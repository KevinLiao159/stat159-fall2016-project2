
## Analysis

```{r, echo=FALSE}
# load data and library
options(warn = -1)
library(xtable)
library(png)
library(grid)
library(Matrix)
library(methods)

# load data in our working environment 
load('../data/ols-regression.RData')
load('../data/ridge-regression.RData')
load('../data/lasso-regression.RData')
load('../data/PCR.RData')
load('../data/PLSR.RData')

# load our functions
source("../code/functions/mse-function.R")
```

#### OLS
First, let's look at our benchmark - OLS regression. We use full set of data that is mean centering and standardizing to fit the OLS model. OLS model will be served as our benchmark for comparison with later four different methods. So we calculate OLS model's MSE, which is equal to `r I(osl_mse)` 
\vspace{40mm}

Here is more information about OLS regression:
```{r, echo=FALSE, results='asis'}
xtable_summary <- xtable(osl_summary, 
                         caption = 'Summary Table of OLS Regression')
print(xtable_summary, comment=FALSE, type = "latex", caption.placement = 'top')
```

Table 1 exhibits estimates of all coefficients and their corresponding t-test and p-value.

From above information, we can conclude that predictors such as *Income*, *Limit*, *Rating*, *Cards*, and *Student* are extremely significant.

#### Ridge Regression

Next, let's look at ridge regression. We load mean centerized and standardized data before the analysis. 
Here are the steps:

1. Check mission value in data for both train and test set
2. Initialize lambda and use random seeds (set.seed()) for cross-validation
3. Use train set to conduct 10-fold cross-validation to find out the best tuning parameter
\begin{figure}[!hb]
  \centering
    \includegraphics[width=70mm]{../images/ridge-cv-lambda.png}
      \caption{the cross-validation errors in terms of the tuning parameter}
\end{figure}

Above is the plot of the cross-validation errors in terms of the tuning parameter.
\vspace{80mm}



4. Find the $\lambda$ for the best model: best $\lambda = `r I(ridge_bestlam)`$
5. Use the test set to compute the test Mean Square Error: test MSE = `r I(ridge_test_mse)`
6. Refit the model on the full data set using the best lambda and get official coefficients.

Here is the official coefficient table:
```{r, echo=FALSE, results='asis', message = FALSE}
colnames(ridge_coef) <- c("Value")
xtable_ridge_coef<- xtable(as.matrix(ridge_coef), 
                         caption = 'Coefficient Table of Ridge Regression')
print(xtable_ridge_coef, comment=FALSE, type = "latex", caption.placement = 'top')

```

#### Lasso Regression
Next, let's look at lasso regression. We load mean centerized and standardized data before the analysis. 
Here are the steps:

1. Check mission value in data for both train and test set
2. Initialize lambda and use random seeds (set.seed()) for cross-validation
3. Use train set to conduct 10-fold cross-validation to find out the best tuning parameter
\begin{figure}[!hb]
  \centering
    \includegraphics[width=70mm]{../images/lasso-cv-lambda.png}
      \caption{the cross-validation errors in terms of the tuning parameter}
\end{figure}

Above is the plot of the cross-validation errors in terms of the tuning parameter.
\vspace{80mm}


4. Find the $\lambda$ for the best model: best $\lambda = `r I(lasso_bestlam)`$
5. Use the test set to compute the test Mean Square Error: test MSE = `r I(lasso_test_mse)`
6. Refit the model on the full data set using the best lambda and get official coefficients.


Here is the official coefficient table:
```{r, echo=FALSE, results='asis'}
colnames(lasso_coef) <- c("Value")
xtable_lasso_coef<- xtable(as.matrix(lasso_coef), 
                         caption = 'Coefficient Table of Lasso Regression')
print(xtable_lasso_coef, comment=FALSE, type = "latex", caption.placement = 'top')
```


#### Principal Components Regression (PCR)
Next, let's look at PCR. We load mean centerized and standardized data before the analysis. 
Here are the steps:

1. Check mission value in data for both train and test set
2. Use random seeds (set.seed()) for cross-validation
3. Use train set to conduct 10-fold cross-validation to find out the best number of principal components used
\begin{figure}[!hb]
  \centering
    \includegraphics[width=70mm]{../images/pcr-cv-ncomp.png}
      \caption{the cross-validation errors in terms of the number of principal components used}
\end{figure}

Above is the plot of the cross-validation errors.
\vspace{80mm}

4. Find the number of principal components considered for the best model: M = `r I(pcr_bestncomp)`
5. Use the test set to compute the test Mean Square Error: test MSE = `r I(pcr_test_mse)`
6. Refit the model on the full data set using M = `r I(pcr_bestncomp)`and get official coefficients.


Here is the official coefficient table:
```{r, echo=FALSE, results='asis'}
colnames(pcr_coef) <- c("Value")
xtable_pcr_coef<- xtable(as.data.frame(pcr_coef), 
                         caption = 'Coefficient Table of PCR')
print(xtable_pcr_coef, comment=FALSE, type = "latex", caption.placement = 'top')
```

#### Partial Least Squares Regression (PLSR)
Next, let's look at PLSR. We load mean centerized and standardized data before the analysis. 
Here are the steps:

1. Check mission value in data for both train and test set
2. Use random seeds (set.seed()) for cross-validation
3. Use train set to conduct 10-fold cross-validation to find out the best number of partial least squares directions
\begin{figure}[!hb]
  \centering
    \includegraphics[width=70mm]{../images/pcr-cv-ncomp.png}
      \caption{the cross-validation errors in terms of the number of partial least squares directions}
\end{figure}

Above is the plot of the cross-validation errors.
\vspace{80mm}

4. Find the number of partial least squares directions for the best model: M = `r I(plsr_bestncomp)`
5. Use the test set to compute the test Mean Square Error: test MSE = `r I(plsr_test_mse)`
6. Refit the model on the full data set using M = `r I(plsr_bestncomp)`and get official coefficients.


Here is the official coefficient table:
```{r, echo=FALSE, results='asis'}
colnames(plsr_coef) <- c("Value")
xtable_plsr_coef<- xtable(as.data.frame(plsr_coef), 
                         caption = 'Coefficient Table of PLSR')
print(xtable_plsr_coef, comment=FALSE, type = "latex", caption.placement = 'top')
```

