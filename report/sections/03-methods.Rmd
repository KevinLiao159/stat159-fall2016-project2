
## Methods

#### Exploratory Data Analysis

First, to better understand the *Credit* dataset, we perform exploratory data analysis. We obtain summary statistics of all the variables, as well as relevant plots to understand the distribution of each variable. We also find the matrix of correlations, scatterplot matrix, ANOVA between `Balance` and the qualitative variables, and a conditional boxplots of `Balance` conditioned to each qualitative variable.

#### Data Processing

Before fitting any of the models, we next conduct some data processing. The qualitative variables, `Gender`, `Student`, `Balance`, and `Ethnicity`, are categorical and thus need to be transformed into dummy variables, or binary indicators, to be used in the regression functions. The variables with two levels, `Gender`, `Student`, and `Married`, have one binary indicator for each, in which each observation takes a value of zero or one. `Ethnicity` has three levels so we create two binary indicators for this variable. We also mean center and standardize the data to remove different measurement scalings and be more comparable. So, each variable has a mean of zero and standard deviation of one.

#### Model Building

There are five regression models to be fitted to the *Credit* dataset, ordinary least squares, ridge, lasso, principal components, and partial least squares. To build the models, we use a training dataset, containing a random sample of 300 out of 400 observations in *Credit*. The remaining 100 observations will be the test set, used to test the performance of the model.

For the non-OLS models, we use ten-fold cross-validation to see how the predictive models would perform in practice. The training set is partitioned into ten equally sized subsamples. One fold is used as the testing set, while the other nine are used to fit the relevant model. This is then repeated for each fold so that all observations are tested exactly once. The cross validation is then used to tune a certain parameter, depending on the model, and then find the value of the parameter that results in the smallest cross validation error. This parameter is then selected as the "best" model, which is then fitted to the test set and finally the full dataset.

#### Regression Models

In order to find the relationship between *Credit* and the ten predictors to be used for predictive modelling, we assume the relationship between the independent and dependent variables is linear. The relationship is assumed to be the following:

$$
\begin{aligned}
Credit_i = \beta_0 + \beta_1X_i + \beta_2X_i + \ldots + \beta_{10}X_i + \epsilon_i
\end{aligned}
$$

Where $\beta_0$ is the intercept and $\beta_1$, ..., $\beta_{10}$ are the regression coefficients for their associated predictor, and $\epsilon$ is the error term. 

We first use an ordinary least squares method to be used as a benchmark for comparison between the other models. OLS estimates the coefficients by minimizing the residual sum of squares (RSS), defined as

$$
\begin{aligned}
RSS = \sum_{i=1}^{n} (y_i - \beta_0 - \sum_{j=1}^{p} \beta_jx_{ij})^2
\end{aligned}
$$

For ridge regression, we minimize the RSS in addition to a shrinkage penalty, defined as

$$
\begin{aligned}
RSS + \lambda \sum_{j=1}^{p} \beta_j^2
\end{aligned}
$$

Where $\lambda$ is the tuning parameter. As $\lambda \rightarrow \infty$, the shrinkage penalty grows, effectively shrinking the coefficients $\beta_1$, ..., $\beta_{10}$ towards zero.

Lasso regression is another shrinkage method like ridge regression. The quantity we want to minimize is defined as

$$
\begin{aligned}
RSS + \lambda \sum_{j=1}^{p} |\beta_j|
\end{aligned}
$$

This is similar to the quantity for ridge regression, except the penalty now contains $|\beta_j|$ instead of $\beta_j^2$. The tuning parameter $\lambda$ is the same, except now sufficiently large $\lambda$ may shrink coefficients to exactly zero. 

For principal components regression, the method tries to reduce the dimensions $X_1$, ..., $X_{p}$ of the data matrix into principal components $Z_1$, ..., $Z_{M}$ and then using the components as the predictors for least squares regression. The $M$ principal compenents will be the tuning parameter, and we use cross validation find which $M$ produces the smallest mean squared error.

Lastly, partial least squares, also a dimension reduction method, tries to find $Z_1$, ..., $Z_{M}$ that approximate the original dimensions like PCR, but also tries to find new features related to the response $Balance$. The tuning parameter, the number of $M$ directions, is the same as well. The $M$ that is associated with the smalled mean squared error will be selected for the model.

Once all of the best models are identified, the test set will be used to compute the MSEs of each, and find which model performs best. The best model will finally be used on the full `Credit` data set to find official coefficients.






